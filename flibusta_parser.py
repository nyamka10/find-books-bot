#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –§–ª–∏–±—É—Å—Ç–∞ (flibusta.is)
"""

import aiohttp
import asyncio
import re
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from urllib.parse import urljoin, quote
import json
import os
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


class FlibustaParser:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –§–ª–∏–±—É—Å—Ç–∞"""
    
    def __init__(self, base_url: str = None):
        self.base_url = base_url or os.getenv('FLIBUSTA_BASE_URL', 'https://flibusta.is')
        self.session = None
        self.is_authenticated = False
        self.user_id = None
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # –¢–∞–π–º–∞—É—Ç—ã
        self.timeout = aiohttp.ClientTimeout(total=30, connect=10)
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.session = aiohttp.ClientSession(headers=self.headers, timeout=self.timeout)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    async def login(self, username: str = None, password: str = None) -> bool:
        """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∞–π—Ç–µ –§–ª–∏–±—É—Å—Ç–∞"""
        username = username or os.getenv('FLIBUSTA_USERNAME')
        password = password or os.getenv('FLIBUSTA_PASSWORD')
        
        if not username or not password:
            print("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return False
        
        try:
            print(f"üîê –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {username}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞
            async with self.session.get(f"{self.base_url}/user/login") as response:
                if response.status != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤—Ö–æ–¥–∞: {response.status}")
                    return False
                
                login_page_text = await response.text()
            
            soup = BeautifulSoup(login_page_text, 'html.parser')
            
            # –ò—â–µ–º —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–∞
            login_form = soup.find('form', {'method': 'post'})
            if not login_form:
                print("‚ùå –§–æ—Ä–º–∞ –≤—Ö–æ–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
            
            # –ü–æ–ª—É—á–∞–µ–º CSRF —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å
            csrf_token = None
            csrf_input = login_form.find('input', {'name': 'csrf_token'})
            if csrf_input:
                csrf_token = csrf_input.get('value')
            
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ö–æ–¥–∞
            login_data = {
                'name': username,
                'pass': password,
                'op': '–í–æ–π—Ç–∏',
                'form_build_id': '',
                'form_id': 'user_login'
            }
            
            if csrf_token:
                login_data['csrf_token'] = csrf_token
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Ö–æ–¥
            async with self.session.post(
                f"{self.base_url}/user/login",
                data=login_data,
                allow_redirects=True
            ) as login_response:
                
                if login_response.status == 200:
                    login_response_text = await login_response.text()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞
                    if 'user' in str(login_response.url) or 'logout' in login_response_text:
                        self.is_authenticated = True
                        print(f"‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")
                        
                        # –ü–æ–ª—É—á–∞–µ–º ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                        await self._get_user_id()
                        
                        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        await asyncio.sleep(1.0)
                        return True
                    else:
                        print("‚ùå –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏–Ω –∏ –ø–∞—Ä–æ–ª—å")
                        return False
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Ö–æ–¥–µ: {login_response.status}")
                    return False
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return False
    
    async def _get_user_id(self):
        """–ü–æ–ª—É—á–∞–µ—Ç ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            async with self.session.get(f"{self.base_url}/user") as response:
                if response.status == 200:
                    profile_text = await response.text()
                    soup = BeautifulSoup(profile_text, 'html.parser')
                    
                    user_link = soup.find('a', href=re.compile(r'/user/\d+'))
                    if user_link:
                        user_id = re.search(r'/user/(\d+)', user_link['href'])
                        if user_id:
                            self.user_id = user_id.group(1)
                            print(f"üÜî ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {self.user_id}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
    
    async def search_books(self, query: str, limit: int = 20) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        try:
            print(f"üîç –ü–æ–∏—Å–∫: '{query}' (–ª–∏–º–∏—Ç: {limit})")
            
            # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è URL
            encoded_query = quote(query.encode('utf-8'))
            search_url = f"{self.base_url}/booksearch?ask={encoded_query}"
            
            async with self.session.get(search_url) as response:
                if response.status != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {response.status}")
                    return []
                
                search_text = await response.text()
            
            # –û—Ç–ª–∞–¥–∫–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            # with open('debug_search.html', 'w', encoding='utf-8') as f:
            #     f.write(search_text)
            # print("üíæ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_search.html")
            
            soup = BeautifulSoup(search_text, 'html.parser')
            books = []
            
            # –ò—â–µ–º —Å–µ–∫—Ü–∏—é "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏"
            books_section = None
            for h3 in soup.find_all('h3'):
                if '–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏' in h3.get_text():
                    books_section = h3.find_next_sibling('ul')
                    break
            
            if books_section:
                book_items = books_section.find_all('li')
                
                for item in book_items[:limit]:
                    try:
                        # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–Ω–∏–≥—É (–ø–µ—Ä–≤–∞—è —Å—Å—ã–ª–∫–∞ —Å /b/)
                        book_link = item.find('a', href=re.compile(r'/b/\d+'))
                        if not book_link:
                            continue
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
                        book_info = {
                            'title': book_link.get_text(strip=True),
                            'book_url': urljoin(self.base_url, book_link['href']),
                            'book_id': re.search(r'/b/(\d+)', book_link['href']).group(1)
                        }
                        
                        # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞ (—Å—Å—ã–ª–∫–∞ —Å /a/)
                        author_links = item.find_all('a', href=re.compile(r'/a/\d+'))
                        if author_links:
                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–≥–æ –∞–≤—Ç–æ—Ä–∞
                            author_link = author_links[0]
                            book_info['author'] = author_link.get_text(strip=True)
                            book_info['author_url'] = urljoin(self.base_url, author_link['href'])
                            book_info['author_id'] = re.search(r'/a/(\d+)', author_link['href']).group(1)
                            
                            # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–≤—Ç–æ—Ä–æ–≤
                            if len(author_links) > 1:
                                authors = []
                                for auth_link in author_links:
                                    authors.append(auth_link.get_text(strip=True))
                                book_info['authors'] = authors
                        
                        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç –ø–æ–¥—Å–≤–µ—Ç–∫–∏ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                        title_clean = re.sub(r'<span[^>]*>|</span>', '', book_info['title'])
                        title_clean = re.sub(r'\s+', ' ', title_clean).strip()
                        book_info['title'] = title_clean
                        
                        if book_info['title']:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
                            books.append(book_info)
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–Ω–∏–≥–∏: {e}")
                        continue
            else:
                print("‚ùå –°–µ–∫—Ü–∏—è '–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                
                # Fallback: –∏—â–µ–º –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–Ω–∏–≥–∏
                all_book_links = soup.find_all('a', href=re.compile(r'/b/\d+'))
                
                for link in all_book_links[:limit]:
                    try:
                        book_info = {
                            'title': link.get_text(strip=True),
                            'book_url': urljoin(self.base_url, link['href']),
                            'book_id': re.search(r'/b/(\d+)', link['href']).group(1)
                        }
                        
                        if book_info['title']:
                            books.append(book_info)
                    except Exception as e:
                        continue
            
            print(f"üìö –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(books)} –∫–Ω–∏–≥")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1.0)
            return books
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _parse_book_item(self, item) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ"""
        try:
            book_info = {}
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
            title_elem = item.find('a', href=re.compile(r'/book/\d+'))
            if title_elem:
                book_info['title'] = title_elem.get_text(strip=True)
                book_info['book_url'] = urljoin(self.base_url, title_elem['href'])
                book_info['book_id'] = re.search(r'/book/(\d+)', title_elem['href']).group(1)
            
            # –ê–≤—Ç–æ—Ä
            author_elem = item.find('a', href=re.compile(r'/author/\d+'))
            if author_elem:
                book_info['author'] = author_elem.get_text(strip=True)
                book_info['author_url'] = urljoin(self.base_url, author_elem['href'])
                book_info['author_id'] = re.search(r'/author/(\d+)', author_elem['href']).group(1)
            
            # –ñ–∞–Ω—Ä—ã
            genres = []
            genre_elems = item.find_all('a', href=re.compile(r'/genre/\d+'))
            for genre_elem in genre_elems:
                genres.append(genre_elem.get_text(strip=True))
            if genres:
                book_info['genres'] = genres
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc_elem = item.find('div', class_='description') or item.find('div', class_='annotation')
            if desc_elem:
                book_info['description'] = desc_elem.get_text(strip=True)
            
            # –†–µ–π—Ç–∏–Ω–≥
            rating_elem = item.find('span', class_='rating') or item.find('div', class_='score')
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                rating_match = re.search(r'(\d+(?:\.\d+)?)', rating_text)
                if rating_match:
                    book_info['rating'] = float(rating_match.group(1))
            
            return book_info if book_info else None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–Ω–∏–≥–∏: {e}")
            return None
    
    async def get_book_details(self, book_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ"""
        try:
            print(f"üìñ –î–µ—Ç–∞–ª–∏ –∫–Ω–∏–≥–∏ ID: {book_id}")
            
            book_url = f"{self.base_url}/b/{book_id}"
            
            async with self.session.get(book_url) as response:
                if response.status != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–Ω–∏–≥–∏: {response.status}")
                    return None
                
                book_text = await response.text()
            
            # –û—Ç–ª–∞–¥–∫–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            # with open(f'debug_book_{book_id}.html', 'w', encoding='utf-8') as f:
            #     f.write(book_text)
            # print(f"üíæ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–Ω–∏–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_book_{book_id}.html")
            
            soup = BeautifulSoup(book_text, 'html.parser')
            book_info = {'book_id': book_id, 'book_url': book_url}
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ - –∏—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title_elem = soup.find('h1', class_='title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # –£–±–∏—Ä–∞–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è (fb2), (epub) –∏ —Ç.–¥.
                title_text = re.sub(r'\s*\([^)]+\)\s*$', '', title_text)
                book_info['title'] = title_text
            else:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π title
                title_elem = soup.find('title')
                if title_elem:
                    title_text = title_elem.get_text(strip=True)
                    if '| –§–ª–∏–±—É—Å—Ç–∞' in title_text:
                        title_text = title_text.split('| –§–ª–∏–±—É—Å—Ç–∞')[0].strip()
                    book_info['title'] = title_text
            
            # –ê–≤—Ç–æ—Ä - –∏—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∞–≤—Ç–æ—Ä–∞ (–ø–µ—Ä–≤–∞—è —Å—Å—ã–ª–∫–∞ /a/)
            author_elem = soup.find('a', href=re.compile(r'/a/\d+'))
            if author_elem:
                book_info['author'] = author_elem.get_text(strip=True)
                book_info['author_url'] = urljoin(self.base_url, author_elem['href'])
                book_info['author_id'] = re.search(r'/a/(\d+)', author_elem['href']).group(1)
            
            # –ñ–∞–Ω—Ä—ã - –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∂–∞–Ω—Ä—ã
            genres = []
            genre_elems = soup.find_all('a', href=re.compile(r'/g/\d+'))
            for genre_elem in genre_elems:
                genre_text = genre_elem.get_text(strip=True)
                if genre_text and len(genre_text) > 2:  # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ
                    genres.append(genre_text)
            if genres:
                book_info['genres'] = genres
            
            # –û–ø–∏—Å–∞–Ω–∏–µ - –∏—â–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
            annotation_elem = soup.find('h2', string='–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è')
            if annotation_elem:
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ "–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è"
                next_elem = annotation_elem.find_next_sibling()
                if next_elem and next_elem.name != 'br':
                    desc_text = next_elem.get_text(strip=True)
                    if desc_text and desc_text != '–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç':
                        book_info['description'] = desc_text
            
            # –°—Å—ã–ª–∫–∏ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã)
            if self.is_authenticated:
                download_links = []
                
                # –ò—â–µ–º select —ç–ª–µ–º–µ–Ω—Ç —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ —Ñ–∞–π–ª–æ–≤
                format_select = soup.find('select', {'id': 'useropt'})
                if format_select:
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                    format_options = format_select.find_all('option')
                    base_download_url = f"{self.base_url}/b/{book_id}/"
                    
                    for option in format_options:
                        format_type = option.get('value', '').upper()
                        if format_type:
                            download_url = base_download_url + format_type.lower()
                            download_links.append({
                                'format': format_type,
                                'url': download_url,
                                'text': f"–°–∫–∞—á–∞—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ {format_type}"
                            })
                
                # –ï—Å–ª–∏ select –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –æ–±—ã—á–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
                if not download_links:
                    download_patterns = [
                        r'/download/\w+',  # /download/fb2, /download/epub –∏ —Ç.–¥.
                        r'/get/\w+',       # /get/fb2, /get/epub –∏ —Ç.–¥.
                        r'/book/\d+/\w+'   # /book/123/fb2, /book/123/epub –∏ —Ç.–¥.
                    ]
                    
                    for pattern in download_patterns:
                        download_elems = soup.find_all('a', href=re.compile(pattern))
                        for elem in download_elems:
                            href = elem.get('href', '')
                            text = elem.get_text(strip=True)
                            
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ URL –∏–ª–∏ —Ç–µ–∫—Å—Ç—É
                            format_match = re.search(r'\.(\w+)$', href) or re.search(r'\.(\w+)$', text)
                            if format_match:
                                format_type = format_match.group(1).upper()
                            else:
                                # –ü–æ–ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Å—ã–ª–∫–∏
                                if 'fb2' in text.lower() or 'fb2' in href.lower():
                                    format_type = 'FB2'
                                elif 'epub' in text.lower() or 'epub' in href.lower():
                                    format_type = 'EPUB'
                                elif 'mobi' in text.lower() or 'mobi' in href.lower():
                                    format_type = 'MOBI'
                                elif 'pdf' in text.lower() or 'pdf' in href.lower():
                                    format_type = 'PDF'
                                elif 'txt' in text.lower() or 'txt' in href.lower():
                                    format_type = 'TXT'
                                else:
                                    format_type = 'UNKNOWN'
                            
                            download_links.append({
                                'format': format_type,
                                'url': urljoin(self.base_url, href),
                                'text': text
                            })
                
                if download_links:
                    book_info['download_links'] = download_links
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1.0)
            return book_info
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π: {e}")
            return None
    
    async def download_book(self, book_id: str, format_type: str = 'epub') -> Optional[bytes]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∫–Ω–∏–≥—É –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        try:
            print(f"‚¨áÔ∏è  –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ {book_id} –≤ —Ñ–æ—Ä–º–∞—Ç–µ {format_type.upper()}")
            
            download_url = f"{self.base_url}/b/{book_id}/{format_type.lower()}"
            
            async with self.session.get(download_url) as response:
                if response.status != 200:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {response.status}")
                    return None
                
                book_content = await response.read()
                print(f"‚úÖ –ö–Ω–∏–≥–∞ —Å–∫–∞—á–∞–Ω–∞: {len(book_content)} –±–∞–π—Ç")
                return book_content
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {e}")
            return None
    
    async def logout(self):
        """–í—ã—Ö–æ–¥ –∏–∑ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            if self.is_authenticated:
                async with self.session.get(f"{self.base_url}/user/logout") as response:
                    self.is_authenticated = False
                    self.user_id = None
                    print("üëã –í—ã—Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ: {e}")


async def search_books(query: str, limit: int = 10):
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–Ω–∏–≥"""
    async with FlibustaParser() as parser:
        # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        if await parser.login():
            print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
            
            # –ü–æ–∏—Å–∫ –∫–Ω–∏–≥
            books = await parser.search_books(query, limit)
            
            if books:
                print(f"\nüìö –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':")
                for i, book in enumerate(books, 1):
                    print(f"\n{i}. {book.get('title', '–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ')}")
                    if book.get('author'):
                        print(f"   üë§ –ê–≤—Ç–æ—Ä: {book['author']}")
                    if book.get('genres'):
                        print(f"   üè∑Ô∏è  –ñ–∞–Ω—Ä—ã: {', '.join(book['genres'][:3])}")
                    if book.get('rating'):
                        print(f"   ‚≠ê –†–µ–π—Ç–∏–Ω–≥: {book['rating']}")
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –ø–µ—Ä–≤–æ–π –∫–Ω–∏–≥–∏
                if books:
                    print(f"\nüìñ –î–µ—Ç–∞–ª–∏ –ø–µ—Ä–≤–æ–π –∫–Ω–∏–≥–∏:")
                    book_details = await parser.get_book_details(books[0]['book_id'])
                    if book_details:
                        print(json.dumps(book_details, ensure_ascii=False, indent=2))
            else:
                print(f"‚ùå –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
            # –í—ã—Ö–æ–¥
            await parser.logout()
        else:
            print("‚ùå –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å!")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ü–∞—Ä—Å–µ—Ä –§–ª–∏–±—É—Å—Ç–∞")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É—á–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not os.getenv('FLIBUSTA_USERNAME') or not os.getenv('FLIBUSTA_PASSWORD'):
        print("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ!")
        print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º:")
        print("FLIBUSTA_USERNAME=–≤–∞—à_–ª–æ–≥–∏–Ω")
        print("FLIBUSTA_PASSWORD=–≤–∞—à_–ø–∞—Ä–æ–ª—å")
        return
    
    # –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    await search_books("—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞", limit=5)


if __name__ == "__main__":
    asyncio.run(main())
